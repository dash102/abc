{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, neighbors, target):\n",
    "    model.train()\n",
    "    model.likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    num_epochs = 1000\n",
    "    for i in range(num_epochs):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(neighbors)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, target)\n",
    "        loss.backward()\n",
    "        # print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        #     i + 1, num_epochs, loss.item(),\n",
    "        #     model.covar_module.base_kernel.lengthscale.item(),\n",
    "        #     model.likelihood.noise.item()\n",
    "        # ))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, neighbors):\n",
    "    model.eval()\n",
    "    model.likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        hi_res_prediction = model.likelihood(model(neighbors))\n",
    "    return hi_res_prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization helpers \n",
    "def _remove_axes(ax):\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "def remove_axes(axes):\n",
    "    if len(axes.shape) == 2:\n",
    "        for ax1 in axes:\n",
    "            for ax in ax1:\n",
    "                _remove_axes(ax)\n",
    "    else:\n",
    "        for ax in axes:\n",
    "            _remove_axes(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(image, kernel_size=15, stride=5):\n",
    "    c = image.shape[1]\n",
    "    patches = nn.Unfold(kernel_size=kernel_size, stride=stride)(image)\n",
    "    b, image_size, grid_size = patches.shape\n",
    "    grid_width, grid_height = int(math.sqrt(grid_size)), int(math.sqrt(grid_size))\n",
    "    patches = patches.reshape(b, c, kernel_size, kernel_size, grid_width, grid_height)\n",
    "    return patches\n",
    "\n",
    "def get_model_inputs(patch):\n",
    "    target = []\n",
    "    neighbors = []\n",
    "    input_kernel_size = 3\n",
    "\n",
    "    b, c, patch_width, patch_height = patch.shape\n",
    "    training = nn.Unfold(kernel_size=input_kernel_size, stride=input_kernel_size)(patch)\n",
    "    training = training.reshape(1, c, input_kernel_size, input_kernel_size, patch_width // 3, patch_height // 3)\n",
    "    b, c, input_width, input_height, grid_width, grid_height = training.shape\n",
    "\n",
    "    for i in range(grid_width):\n",
    "        for j in range(grid_height):\n",
    "            y = training[:, :, 1, 1, i, j]\n",
    "            X_NL = training[:, :, :, :, i, j]\n",
    "            X_NL = X_NL.reshape(c, 1, 9).permute(1, 2, 0)\n",
    "            X_NL = torch.cat([X_NL[:, :4, :], X_NL[:, 5:, :]], dim=1)\n",
    "\n",
    "            target.append(y.squeeze())\n",
    "            neighbors.append(X_NL.squeeze())\n",
    "\n",
    "    target = torch.stack(target)\n",
    "    neighbors = torch.stack(neighbors)\n",
    "\n",
    "    return neighbors, target\n",
    "\n",
    "def get_neighbors_per_pixel(patch, kernel_size=3, stride=1, padding=1):\n",
    "    b, c, width, height = patch.shape\n",
    "    padded_patch = nn.ReflectionPad2d(padding)(patch)\n",
    "    neighbors = nn.Unfold(kernel_size=kernel_size, stride=stride)(padded_patch)\n",
    "    neighbors = neighbors.reshape(b, 9, (width + padding*2 - kernel_size + stride)**2)\n",
    "    neighbors = torch.cat([neighbors[:, :4, :], neighbors[:, 5:, :]], dim=1).permute(0, 2, 1)\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blending patches together to get final image\n",
    "\n",
    "def blend_patch(x0, x1, overlap, direction):\n",
    "    xh0, xw0 = x0.shape\n",
    "    xh1, xw1 = x1.shape\n",
    "    dim_to_overlap = xw0\n",
    "    other_dim = xh0\n",
    "    \n",
    "    if direction == 'v':\n",
    "        x0 = x0.permute(1, 0)\n",
    "        x1 = x1.permute(1, 0)\n",
    "        dim_to_overlap = xh0\n",
    "        other_dim = xw0\n",
    "\n",
    "    xramp0 = dim_to_overlap - overlap\n",
    "    xramp1 = 0\n",
    "\n",
    "    x00 = x0[:, :xramp0]\n",
    "    x01 = x0[:, xramp0:]\n",
    "\n",
    "    x11 = x1[:, :overlap]\n",
    "    x12 = x1[:, overlap:]\n",
    "\n",
    "    ramp0 = torch.linspace(1, 0, overlap).repeat((other_dim, 1))\n",
    "    ramp1 = 1 - ramp0\n",
    "\n",
    "    blend_mid = x01 * ramp0 + x11 * ramp1\n",
    "\n",
    "    blend = torch.cat([x00, blend_mid, x12], dim=1)\n",
    "    \n",
    "    if direction == 'v':\n",
    "        blend = blend.permute(1, 0)\n",
    "    return blend\n",
    "\n",
    "def blend(patches, overlap, dims):\n",
    "    h, w = dims\n",
    "    curr_blend = None\n",
    "    \n",
    "    for i in range(h):\n",
    "        row_blend = patches[i * w]\n",
    "        for j in range(1, w):\n",
    "            if w == 0 and h == 0:\n",
    "                continue\n",
    "            \n",
    "            idx = i * w + j\n",
    "\n",
    "            next_patch = patches[idx]\n",
    "            row_blend = blend_patch(row_blend, next_patch, overlap, 'h')\n",
    "\n",
    "        if curr_blend is None:\n",
    "            curr_blend = row_blend\n",
    "        else:\n",
    "            curr_blend = blend_patch(curr_blend, row_blend, overlap, 'v')\n",
    "    return curr_blend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsample implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(L, kernel_size=15, stride=5, scale_factor=10):\n",
    "    print(\"UPSAMPLING\")\n",
    "    patches_L = get_patches(L, kernel_size=kernel_size, stride=stride)\n",
    "    Hb = nn.Upsample(scale_factor=scale_factor, mode='bicubic')(L)\n",
    "    Hb_patches = get_patches(Hb, kernel_size * scale_factor, stride=stride * scale_factor)\n",
    "\n",
    "    preds = []\n",
    "    for i in range(patches_L.shape[-1]):\n",
    "        for j in range(patches_L.shape[-1]):\n",
    "            # print(i, j)\n",
    "            patch_L = patches_L[..., i, j]\n",
    "            Xnl, y = get_model_inputs(patch_L)\n",
    "\n",
    "            likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "            model = ExactGPModel(Xnl, y, likelihood)\n",
    "            train(model, Xnl, y)\n",
    "            \n",
    "            Hb_patch = Hb_patches[..., i, j]\n",
    "            inference = get_neighbors_per_pixel(Hb_patch)\n",
    "            observed_pred = predict(model, inference)\n",
    "            \n",
    "            s = int(math.sqrt(observed_pred.loc.shape[-1]))\n",
    "            preds.append(observed_pred.loc.reshape(s, s))\n",
    "\n",
    "    s = Hb_patches.shape[-1]\n",
    "    blended = blend(preds, overlap=(kernel_size - stride) * scale_factor, dims=(s, s))\n",
    "    return blended.unsqueeze(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deblur implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deblur(Hb, L, Lb):\n",
    "    print(\"DEBLURRING\")\n",
    "    # partition downsampled_upsampled_low_res into n overlapped patches P_1, ..., P_n corresponding to those in low_res\n",
    "    hi_res_stride = 8\n",
    "    low_res_patches, hi_res_patches = get_patches(Lb), get_patches(Hb, stride=hi_res_stride)\n",
    "    batch_size, num_channels, patch_width, patch_height, grid_width, grid_height = low_res_patches.shape\n",
    "\n",
    "    hi_res_predictions_patches = []\n",
    "    # for each patch, build y & X_NL and train a GPR model on them\n",
    "    for i in range(grid_width):\n",
    "        for j in range(grid_height):\n",
    "            low_res_patch = low_res_patches[:, :, :, :, i, j]\n",
    "            hi_res_patch = hi_res_patches[:, :, :, :, i, j]\n",
    "            train_neighbors, train_target = get_model_inputs(low_res_patch)\n",
    "            test_neighbors = get_neighbors_per_pixel(hi_res_patch)\n",
    "\n",
    "            # train a GPR model M using {target, neighbors}\n",
    "            likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "            model = ExactGPModel(train_neighbors, train_target, likelihood)\n",
    "            train(model, train_neighbors, train_target)\n",
    "\n",
    "            # for each pixel of hi_res_patch, get its eight neighbors and predict the new hi_res pixel\n",
    "            hi_res_prediction_patch = predict(model, test_neighbors)\n",
    "            hi_res_patch_width, hi_res_patch_height = hi_res_patch.shape[1], hi_res_patch.shape[2]\n",
    "            hi_res_predictions_patches.append(hi_res_prediction_patch.loc.squeeze().reshape(hi_res_patch_width, hi_res_patch_height))\n",
    "\n",
    "    # p_H <- M(X_NH)\n",
    "    # hi_res_predictions = torch.stack(hi_res_predictions_patches, dim=1).reshape(batch_size, num_channels, patch_width, patch_height, grid_width, grid_height)\n",
    "    # folded_prediction = F.fold(hi_res_predictions.reshape((1,225,81)), Hb.shape[-2:], kernel_size=15, stride=8)\n",
    "    blended_H = blend(hi_res_predictions_patches, overlap=hi_res_stride, dims=(grid_width,grid_height))\n",
    "    return blended_H"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRGPR on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm:\n",
    "# Hb <- upsample(L)\n",
    "# Lb <- Hb blur and downsample\n",
    "# H <- Deblur(Hb, L, Lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPSAMPLING\n"
     ]
    }
   ],
   "source": [
    "scale_factor = 10\n",
    "\n",
    "img = Image.open('./images/low_res_circles.png').convert(\"L\")\n",
    "L = T.Compose([\n",
    "    T.Resize(50),\n",
    "    T.ToTensor()\n",
    "])(img).unsqueeze(0)\n",
    "Hb = upsample(L, scale_factor=scale_factor)\n",
    "Lb = T.GaussianBlur(11)(Hb)\n",
    "Lb = F.interpolate(Lb.unsqueeze(0), scale_factor=(1/scale_factor), mode='bilinear', antialias=True)\n",
    "H = deblur(Hb.unsqueeze(0), L, Lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 50, 50])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# plt.imshow(Hb.detach().numpy().squeeze())\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# plt.imshow(Lb.squeeze())\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Hb.shape\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Lb = T.GaussianBlur(11)(Hb.unsqueeze(0))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Lb.shape\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(Lb\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 7\u001b[0m get_patches(Hb, stride\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m) \u001b[39m# shape [550, 550]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mget_patches\u001b[0;34m(image, kernel_size, stride)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_patches\u001b[39m(image, kernel_size\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[1;32m      2\u001b[0m     c \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m     patches \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mUnfold(kernel_size\u001b[39m=\u001b[39;49mkernel_size, stride\u001b[39m=\u001b[39;49mstride)(image)\n\u001b[1;32m      4\u001b[0m     b, image_size, grid_size \u001b[39m=\u001b[39m patches\u001b[39m.\u001b[39mshape\n\u001b[1;32m      5\u001b[0m     grid_width, grid_height \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(math\u001b[39m.\u001b[39msqrt(grid_size)), \u001b[39mint\u001b[39m(math\u001b[39m.\u001b[39msqrt(grid_size))\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a/lib/python3.9/site-packages/torch/nn/modules/fold.py:298\u001b[0m, in \u001b[0;36mUnfold.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49munfold(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation,\n\u001b[1;32m    299\u001b[0m                     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a/lib/python3.9/site-packages/torch/nn/functional.py:4669\u001b[0m, in \u001b[0;36munfold\u001b[0;34m(input, kernel_size, dilation, padding, stride)\u001b[0m\n\u001b[1;32m   4665\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[1;32m   4666\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   4667\u001b[0m         unfold, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, kernel_size, dilation\u001b[39m=\u001b[39mdilation, padding\u001b[39m=\u001b[39mpadding, stride\u001b[39m=\u001b[39mstride\n\u001b[1;32m   4668\u001b[0m     )\n\u001b[0;32m-> 4669\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mim2col(\u001b[39minput\u001b[39;49m, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride))\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "# plt.imshow(Hb.detach().numpy().squeeze())\n",
    "# plt.imshow(Lb.squeeze())\n",
    "# Hb.shape\n",
    "# Lb = T.GaussianBlur(11)(Hb.unsqueeze(0))\n",
    "# Lb.shape\n",
    "# print(Lb.shape)\n",
    "# get_patches(Hb, stride=8) # shape [550, 550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "ax[0].imshow(L.squeeze())\n",
    "ax[1].imshow(Hb.detach().numpy().squeeze())\n",
    "ax[2].imshow(Lb.squeeze())\n",
    "ax[3].imshow(H.detach().numpy().squeeze())\n",
    "\n",
    "ax[0].set_title('LR (45x45)')\n",
    "ax[1].set_title('Bicubic')\n",
    "ax[2].set_title('Lb')\n",
    "ax[2].set_title('H')\n",
    "\n",
    "remove_axes(ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abc",
   "language": "python",
   "name": "abc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
